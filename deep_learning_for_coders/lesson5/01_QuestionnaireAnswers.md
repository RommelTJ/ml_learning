# Questionnaire Answers

1. Does ethics provide a list of "right answers"?

There is no list of do's and dont's. Ethics is complicated, and context-dependent. 
It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. 
In this chapter, our goal is to provide some signposts to help you on that journey.

2. How can working with people of different backgrounds help when considering ethical questions?

Different people's backgrounds will help them to see things which may not be obvious to you. Working with a team is 
helpful for many muscle building activities, including this one.

3. What was the role of IBM in Nazi Germany? Why did the company participate as it did? Why did the workers participate?

IBM supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other 
groups on a massive scale.

4. What was the role of the first person jailed in the Volkswagen diesel scandal?

It was one of the engineers, James Liang, who just did what he was told.

5. What was the problem with a database of suspected gang members maintained by California law enforcement officials?

A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, 
including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked 
as "admitting to being gang members"). In this case, there was no process in place for correcting mistakes or 
removing people once theyâ€™d been added.

6. Why did YouTube's recommendation algorithm recommend videos of partially clothed children to pedophiles, even 
though no employee at Google had programmed this feature?

The problem here is the centrality of metrics in driving a financially important system. When an algorithm has a 
metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to 
all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases 
and feedback loops for their advantage.

7. What are the problems with the centrality of metrics?

When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. 
This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and 
exploit these edge cases and feedback loops for their advantage.

8. Why did Meetup.com not include gender in its recommendation system for tech meetups?

Meetup had observed that men expressed more interest than woman towards attending Tech meetups.
They were concerned that including gender in the recommendation algorithm would create a self-reinforcing feedback 
loop where it would recommend Tech meetups mainly to men.
To avoid this situation and continue to recommend Tech meetups to their users regardless of the gender, they 
simply decided to not include gender in the recommendation algorithm.

9. What are the six types of bias in machine learning, according to Suresh and Guttag?

Different types of bias:
* Historical bias
* Representation bias
* Measurement bias
* Aggregation bias
* Evaluation bias
* Deployment bias

10. Give two examples of historical race bias in the US.

When doctors were shown identical files, they were much less likely to recommend cardiac catherization (a 
helpful procedure) to Black patients.

An all-white jury was 16% more likely to convict a Black defendant than a white one, but when a jury had at 
least one Black member, it convicted both at the same rate.

11. Where are most images in ImageNet from?

The US and other Western countries.

12. In the paper Does Machine Learning Automate Moral Hazard and Error why is sinusitis found to be predictive of a stroke?

Measurement Bias
* Ex: Trying to predict strokes, they look at metrics to predict strokes.
* Many of those metrics are from high utility patients vs low utility patients.
* People that utilize healthcare a lot will go to the doctor for Sinusitis and Stroke.

13. What is representation bias?

When the model emphasize some property of the data as it seemingly has the closest correlation with the prediction, 
even though that might not be the truth.

An example is the gender property in the occupation prediction model where the model only predicted 11.6% of 
surgeons to be women whereas the real number was 14.6%.

14. How are machines and people different, in terms of their use for making decisions?

* Humans use people and algorithms differently when getting advice on decisions.
* People assume that algorithms are objective or/and error-free.
* Algorithms are more likely to be implemented with a no-appeals process in place. 
* Algorithms are often used at scale. 
* Algorithmic systems are cheap.

15. Is disinformation the same as "fake news"?

Disinformation has a history stretching back hundreds or even thousands of years. It is not necessarily about 
getting someone to believe something false, but rather often used to sow disharmony and uncertainty, and to get 
people to give up on seeking the truth.

To do that disinformation often contain exaggerations, seeds of truth or half-truths taken out of context rather 
than just "fake news".

16. Why is disinformation through auto-generated text a particularly significant issue?

Disinformation through autogenerated text is a particularly significant issue, due to the greatly increased 
capability provided by deep learning.

17. What are the five ethical lenses described by the Markkula Center?

The objective of looking through different ethical lenses when making a decision is to uncover concrete issues with 
the different options. 

The lenses are:
* The rights approach
* The justice approach
* The utilitarian approach
* The common good approach
* The virtue approach

18. Where is policy an appropriate tool for addressing data ethics issues?

Policies are an appropriate tool for addressing data ethics issues when is likely that design fixes, 
self regulation and technical approaches to addressing problems, involving ethical uses of 
Machine Learning are not working.

While such measures can be useful, they will not be sufficient to address the underlying problems that have led 
to our current state. For example, as long as it is incredibly profitable to create addictive technology, 
companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories 
and polluting our information ecosystem. While individual designers may try to tweak product designs, we will 
not see substantial changes until the underlying profit incentives changes.

Because of the above it is almost certain that policies will have to be created by government to address these issues.
